# -*- coding: utf-8 -*-
"""Sentiment analysis thesis 2nd april.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1bfLm_byMkuMs1IIr3v2OgBcCT5y3V-ZE
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import string
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn.neighbors import KNeighborsClassifier
from sklearn.neural_network import MLPClassifier
from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import Pipeline
from sklearn.model_selection import GridSearchCV
import nltk
from nltk.stem import WordNetLemmatizer
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
import gensim.downloader as api
import json
import joblib
import os
from wordcloud import WordCloud
import re

random_seed = 42  # Answer to the Ultimate Question of Life, the Universe, and the seed for this program

# Download NLTK data
nltk.download('punkt')
nltk.download('wordnet')
nltk.download('stopwords')

if __name__ == "__main__":
    try:
        # Check if the directory already exists
        if not os.path.isdir('IMDB-Movie-Reviews-Large-Dataset-50k-master'):
            # Clone the repository
            #!git clone https://github.com/laxmimerit/IMDB-Movie-Reviews-Large-Dataset-50k.git
            print("IMDB dataset doesn't exist")
        else:
            print("Directory 'IMDB-Movie-Reviews-Large-Dataset-50k' already exists. Skipping cloning.")

        # Load data
        df = pd.read_excel('IMDB-Movie-Reviews-Large-Dataset-50k-master/train.xlsx', names=["text", "score"])
        df_test = pd.read_excel('IMDB-Movie-Reviews-Large-Dataset-50k-master/test.xlsx', names=["text", "score"])
    except:
        print("There was an error in uploading/finding the dataset or loading it into memory.")


def wordCloud(df):

  # Convert values to strings
  df['text'] = df['text'].astype(str)
  # Combine all text into a single string
  text = ' '.join(df['text'])

  # Create the WordCloud object
  wordcloud = WordCloud(width=800, height=400, background_color='white').generate(text)

  # Plot the WordCloud
  plt.figure(figsize=(10, 6))
  plt.imshow(wordcloud, interpolation='bilinear')
  plt.axis('off')  # Hide axis
  plt.show()

#wordCloud(df)

# Preprocessing

lemmatizer = WordNetLemmatizer()
stop_words = set(stopwords.words('english'))

def preprocessing_text(text):
   
    # Set to lowercase
    text=text.lower()
  
    # Remove punctuation
    special_chars = "!@#$%^&*()_+=-[]{}|;:',.<>?~`"
    for character in special_chars:
        text = text.replace(character, '')

    # Remove numbers
    for character in "0123456789":
        text = text.replace(character, '')

    text=remove_stopwords_and_lemmatize(text)
    return text


def preprocessing(df):
    # Set to lowercase
    df["text"] = df["text"].str.lower()
  
    # Remove punctuation
    special_chars = "!@#$%^&*()_+=-[]{}|;:',.<>?~`"
    for character in special_chars:
        df["text"] = df["text"].str.replace(character, '', regex=False)

    # Remove numbers
    for character in "0123456789":
        df["text"] = df["text"].str.replace(character, '', regex=True)

    df['text'] = df['text'].apply(remove_stopwords_and_lemmatize)
    return df



def remove_stopwords_and_lemmatize(sentence):
    if isinstance(sentence, str):
        word_list = word_tokenize(sentence)
        filtered_sentence = [word for word in word_list if word not in stop_words]
        filtered_sentence = ' '.join(filtered_sentence)
        lemmatized_output = ''.join([lemmatizer.lemmatize(w) for w in filtered_sentence])
        return lemmatized_output
    else:
        return ""


if __name__ == "__main__":
    #Preprocessing both train and test datasets
    df=preprocessing(df)
    df_test=preprocessing(df_test)

    #fitting the vectorizer to the corpus
    vectorizer = CountVectorizer()
    vectorizer.fit(df["text"])

    try:
        # Save the vectorizer to a file
        joblib.dump(vectorizer, 'count_vectorizer.joblib')

    except:
        print("There was an error in saving the vectorizer.")





# Bag of Words
def convertSingleToBOG(text,vectorizer):
    # Convert the single string to a list with a single element
    text_list = [text]
    
    # Transform the list of a single document
    X_bog = vectorizer.transform(text_list)
    return X_bog

def convertToBOG(df):
  X_bog = vectorizer.transform(df["text"])
  return X_bog

if __name__ == "__main__":
    #Converting to bog both train and test datasets
    X_bog=convertToBOG(df)
    X_test_bog=convertToBOG(df_test)

'''
# Word2Vec
#w2v_model = api.load("word2vec-google-news-300")

def get_vector(word):
    try:
        return w2v_model[word]
    except KeyError:
        return np.zeros(w2v_model.vector_size)

def document_w2v(comment):
    # Tokenize the comment
    tokenized_comment = word_tokenize(comment)

    # Get vectors for each word in the comment
    word_vectors = [get_vector(word) for word in tokenized_comment]

    # Filter out empty word vectors
    word_vectors = [vector for vector in word_vectors if vector.any()]

    if not word_vectors:
        # If no valid word vectors found, return a zero vector
        return np.zeros(w2v_model.vector_size)

    # Average the word vectors to get the comment vector
    comment_vector = np.mean(word_vectors, axis=0)

    return comment_vector

def convertToW2v(df):
  #Convert to w2v vector and then convert it to numpy array
  X_w2v = np.array(df['text'].apply(document_w2v).tolist())
  return X_w2v


#Converting to w2v both train and test datasets
X_w2v=convertToW2v(df)
X_test_w2v=convertToW2v(df_test)
'''
if __name__ == "__main__":
    # Splitting data

    #This is how splitting is normally done however in this database the train and test datasets were already separated
    x_bog_train, x_bog_test, y_bog_train, y_bog_test = train_test_split(X_bog, df["score"], test_size=0.8,random_state=random_seed)
    #x_w2v_train, x_w2v_test, y_w2v_train, y_w2v_test = train_test_split(X_w2v, df["score"], test_size=0.8,random_state=random_seed)
    #x_bog_train=X_bog
    #x_bog_test=X_test_bog
    #y_bog_train=df['score']
    #y_bog_test=df_test['score']

'''
x_w2v_train=X_w2v
x_w2v_test=X_test_w2v
y_w2v_train=df['score']
y_w2v_test=df_test['score']
'''
if __name__ == "__main__":
    # Create a pipeline dictionary with different models
    pipelines = {
        'svm': Pipeline([('svm', SVC(random_state=random_seed))]),
        'nb': Pipeline([('nb', MultinomialNB())]),
        'k-neighbors': Pipeline([('k-neighbors', KNeighborsClassifier())]),
        'random_forest': Pipeline([('rf', RandomForestClassifier(random_state=random_seed))]),
        'logistic_regression': Pipeline([('lr', LogisticRegression(random_state=random_seed))])
    }

    # Parameters grid for each model
    params = {
        'svm': {
            'svm__C': [0.1, 1, 10],
            'svm__kernel': ['linear', 'rbf']
        },
        'nb': {
            'nb__alpha': [0.1, 1, 10],
            'nb__fit_prior': [True, False]
        },
        'k-neighbors': {
            'k-neighbors__n_neighbors': [3, 5, 7],
            'k-neighbors__weights': ['uniform', 'distance'],
            'k-neighbors__p': [1, 2]
        },
        'random_forest': {
            'rf__n_estimators': [50, 100, 200],
            'rf__max_depth': [5, 10,50,100,200]
        },
        'logistic_regression': {
            'lr__C': [0.1, 1, 10],
            'lr__solver': ['liblinear', 'lbfgs']
        }
    }

    # Create GridSearchCV for all models with parameters
    grid_searches = {name: GridSearchCV(model, params[name], cv=5, n_jobs=-1, scoring="accuracy")
                    for name, model in pipelines.items()}


    # Fit the GridSearchCV for all models using BOG
    print("Using BOG data:")
    for name, grid_search in grid_searches.items():
        print(f"Training {name}...")
        grid_search.fit(x_bog_train, y_bog_train)
        print(f"Best {name} Parameters: {grid_search.best_params_}")
        print(f"Best {name} Accuracy: {grid_search.best_score_}\n")

'''
# Fit the GridSearchCV for all models using w2v
print("Using w2v data:")
for name, grid_search in grid_searches.items():
  if name != 'nb':  # Skip Naive Bayes for Word2Vec because it has negative numbers that Nb cannot use
    print(f"Training {name}...")
    grid_search.fit(x_w2v_train, y_w2v_train)
    print(f"Best {name} Parameters: {grid_search.best_params_}")
    print(f"Best {name} Accuracy: {grid_search.best_score_}\n")


#Save results to json to be read in the dashboard
# Convert grid_searches to JSON-compatible dictionary
grid_searches_dict = {name: gs.cv_results_ for name, gs in grid_searches.items()}

# Save as JSON
with open('grid_searches_results.json', 'w') as json_file:
    json.dump(grid_searches_dict, json_file, indent=4)
'''

""" # Convert tuples to NumPy arrays
x_w2v_train_array = np.array(x_w2v_train.tolist())
y_w2v_train_array = np.array(y_w2v_train.tolist())

# Check the shapes
print("Shape of x_w2v_train_array:", x_w2v_train_array.shape)
print("Shape of y_w2v_train_array:", y_w2v_train_array.shape)

# Check the types
print("type of x_w2v_train_array:", type(x_w2v_train_array))
print("type of y_w2v_train_array:", type(y_w2v_train_array)) """

""" # Fit the GridSearchCV for all models using w2v
print("Using w2v data:")
for name, grid_search in grid_searches.items():
  if name != 'nb':  # Skip Naive Bayes for Word2Vec because it has negative numbers that Nb cannot use
    print(f"Training {name}...")
    grid_search.fit(x_w2v_train, y_w2v_train)
    print(f"Best {name} Parameters: {grid_search.best_params_}")
    print(f"Best {name} Accuracy: {grid_search.best_score_}\n") """

# Convert numpy arrays in cv_results to lists
def convert_numpy(obj):
    if isinstance(obj, np.ndarray) or isinstance(obj, np.ma.core.MaskedArray):
        return obj.tolist()
    elif isinstance(obj, list):
        return [convert_numpy(item) for item in obj]
    return obj

# Save GridSearchCV results to JSON
def save_results(grid_searches,results_path):
  
  results_dict = {name: {
      "best_params": grid_search.best_params_,
      "best_score": grid_search.best_score_,
      "test_score": accuracy_score(grid_search.best_estimator_.predict(x_bog_test),y_bog_test),
      "cv_results": {key: convert_numpy(value) for key, value in grid_search.cv_results_.items()}
  } for name, grid_search in grid_searches.items()}

  with open(results_path, "w") as results_json:
      json.dump(results_dict, results_json, indent=4)

if __name__ == "__main__":
    #Save the results to file
    results_path = "grid_search_results.json"
    save_results(grid_searches,results_path)

# Function to evaluate models and save results
def evaluate_and_save_results(grid_searches, X_test, y_test, results_file):
    evaluation_results = {}
    for name, grid_search in grid_searches.items():
        best_model = grid_search.best_estimator_

        # Evaluate the model
        y_pred = best_model.predict(X_test)
        accuracy = accuracy_score(y_test, y_pred)
        class_report = classification_report(y_test, y_pred)
        conf_matrix = confusion_matrix(y_test, y_pred)

        # Update the cv_results
        cv_results = grid_search.cv_results_
        cv_results["test_accuracy"] = accuracy
        cv_results["classification_report"] = class_report
        cv_results["confusion_matrix"] = conf_matrix.tolist()
        cv_results["test_score"] = accuracy  # Include test score

        # Convert numpy arrays and masked arrays to lists
        cv_results = {key: convert_numpy(value) for key, value in cv_results.items()}

        # Save evaluation results
        evaluation_results[name] = {
            "best_params": grid_search.best_params_,
            "best_score": grid_search.best_score_,
            "cv_results": cv_results
        }

      # Print keys and types of values in cv_results
    for key, value in cv_results.items():
        print(key, type(value))

    # Save evaluation results to JSON file
    with open(results_file, "w") as results_json:
        json.dump(evaluation_results, results_json, indent=4)

if __name__ == "__main__":
    try:
        # Define the path
        evaluation_results_file = "evaluation_results_with_test_score.json"

        # Evaluate models with test data and save results
        evaluate_and_save_results(grid_searches, X_test_bog, df_test["score"], evaluation_results_file)
    
    except:
        print("There was an error in saving the results of the model training.")

# Saves the best models to json. Needs grid searches and a location for the json as input
def save_models_to_binary(grid_searches, output_path):
    # Create the folder if it doesn't exist
    if not os.path.exists(os.path.dirname(output_path)):
        os.makedirs(os.path.dirname(output_path))

    models = {}
    for name, grid_search in grid_searches.items():
        models[name] = grid_search.best_estimator_

    with open(output_path, "wb") as json_file:
        joblib.dump(models, json_file)

if __name__ == "__main__":
    try:
        # Define the output path for saving the best models
        best_models_path = "best_models/best_models.joblib"

        # Save the best models
        save_models_to_binary(grid_searches, best_models_path)
    except:
        print("There was an error in saving the models to file.")


#Loads models from json to a dictionary. Needs a location path
def load_models_from_joblib(path):
    try:
        with open(path, "rb") as json_file:
            models = joblib.load(json_file)

            # Print the names of the loaded models
            print("Models Loaded:")
            for name in models:
               print("- ", name)
        return models
    except Exception as e:
        print(f"Error loading models from {path}: {e}")
        return None